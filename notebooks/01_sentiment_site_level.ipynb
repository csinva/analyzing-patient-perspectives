{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import os.path\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import imodelsx\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import imodelsx.viz\n",
    "import json\n",
    "import seaborn as sns\n",
    "import data\n",
    "import joblib\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy.stats import spearmanr\n",
    "LEVELS = ['Very Negative', 'Negative', 'Neutral',\n",
    "          'No response', 'Positive', 'Very Positive']\n",
    "files_dict = data.load_files_dict_single_site()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter data for single-site analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = 'meta-llama/Llama-2-7b-hf'\n",
    "# checkpoint = 'meta-llama/Llama-2-70b-hf'\n",
    "checkpoint = 'gpt-4'  # gpt-35-turbo\n",
    "# checkpoint = 'gpt-35-turbo'\n",
    "# checkpoint = 'mistralai/Mistral-7B-v0.1'\n",
    "\n",
    "\n",
    "# site = 'Atlanta'\n",
    "# site = 'Columbus'\n",
    "# site = 'WashingtonDC'\n",
    "site = 'Charlotte'  # Houston, Portland\n",
    "df = files_dict[site]\n",
    "qs, responses_df, themes_df = data.split_single_site_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run sentiment\n",
    "Note: this uses a lot of API calls (num questions * num responses), maybe around 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sentiment(site, checkpoint, qs, responses_df, themes_df, llm=None):\n",
    "    sentiment_prompt = '''### You are given a question and a response. Rate the sentiment/supportiveness of the response on a scale of 1 to 5, where 1 is very negative and 5 is very positive. ###\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Response: {response}\n",
    "\n",
    "Rating (1-5):'''\n",
    "    if llm is None:\n",
    "        llm = imodelsx.llm.get_llm(checkpoint, repeat_delay=None)\n",
    "\n",
    "    num_questions = len(qs)\n",
    "    sentiments = defaultdict(list)\n",
    "    for question_num in tqdm(range(num_questions), position=0):\n",
    "        question, responses, theme_dict = data.get_data_for_question_single_site(\n",
    "            question_num=question_num, qs=qs, responses_df=responses_df, themes_df=themes_df)\n",
    "\n",
    "        for response_num in tqdm(range(len(responses)), position=1):\n",
    "            response = responses.values[response_num]\n",
    "\n",
    "            if pd.isna(response):\n",
    "                sentiments[question_num].append(np.nan)\n",
    "            else:\n",
    "                prompt = sentiment_prompt.format(\n",
    "                    question=question, response=response)\n",
    "                ans = llm(prompt)\n",
    "                sentiments[question_num].append(ans)\n",
    "\n",
    "    # save\n",
    "    pd.DataFrame(sentiments).T.to_pickle(\n",
    "        join(data.PROCESSED_DIR, f'sentiments_df_{site}_{checkpoint.split(\"/\")[-1]}.pkl'))\n",
    "    return sentiments\n",
    "\n",
    "\n",
    "def compute_and_save_sent_df(sentiments, site, checkpoint):\n",
    "\n",
    "    sent_df = pd.DataFrame([(key, var) for (key, L) in sentiments.items() for var in L],\n",
    "                           columns=['Question', 'Value'])\n",
    "\n",
    "    # round  values\n",
    "    sent_df['Value'] = sent_df['Value'].astype(float).round()\n",
    "    value_maps = {\n",
    "        1: 'Very Negative',\n",
    "        2: 'Negative',\n",
    "        3: 'Neutral',\n",
    "        4: 'Positive',\n",
    "        5: 'Very Positive',\n",
    "    }\n",
    "    sent_df['Value'] = sent_df['Value'].map(value_maps.get)\n",
    "    sent_df['Value'] = sent_df['Value'].fillna('No response')\n",
    "\n",
    "    sent_df = sent_df.groupby(\n",
    "        ['Question', 'Value']).size().unstack(fill_value=0)\n",
    "    sent_df = sent_df.reindex(LEVELS, axis=1)\n",
    "    if checkpoint == 'gpt-4':\n",
    "        joblib.dump(sent_df, join(data.PROCESSED_DIR,\n",
    "                    f'sent_df_{site}_gpt-4.pkl'))\n",
    "    return sent_df\n",
    "\n",
    "# run single\n",
    "# sentiments = run_sentiment(site, checkpoint, qs, responses_df, themes_df)\n",
    "\n",
    "\n",
    "# run all sites\n",
    "llm = imodelsx.llm.get_llm(checkpoint, repeat_delay=20)\n",
    "# ['Atlanta', 'Columbus', 'WashingtonDC']:\n",
    "# for site in ['Charlotte', 'Houston', 'Portland']:\n",
    "for site in ['Dallas', 'Seattle', 'Tucson']:\n",
    "    df = files_dict[site]\n",
    "    qs, responses_df, themes_df = data.split_single_site_df(df)\n",
    "    sentiments = run_sentiment(\n",
    "        site, checkpoint, qs, responses_df, themes_df, llm=llm)\n",
    "    sent_df = compute_and_save_sent_df(sentiments, site, checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make plot\n",
    "sent_df = compute_and_save_sent_df(sentiments, site, checkpoint)\n",
    "sent_df = sent_df.sort_values(by=LEVELS, ascending=False)\n",
    "colors = sns.diverging_palette(20, 220, n=6).as_hex()\n",
    "colors = colors[:2] + ['#ddd', '#eee'] + colors[-2:]\n",
    "sent_df.plot(kind='barh', stacked=True, figsize=(5, 10), color=colors)\n",
    "\n",
    "plt.yticks(range(46), labels=df['Domain'].values[sent_df.index.values])\n",
    "plt.legend(bbox_to_anchor=(0.5, 1.1), loc='center', ncol=3, title='Sentiment')\n",
    "plt.xlabel('Answer count')\n",
    "plt.ylabel('Question number and domain')\n",
    "plt.title(site)\n",
    "plt.savefig(f'../figs/eda/sentiment_example_{site}.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate sentiment plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dfs = []\n",
    "#\n",
    "sites = ['Atlanta', 'Columbus', 'WashingtonDC'] + \\\n",
    "    ['Charlotte', 'Houston', 'Portland'] + \\\n",
    "    ['Dallas', 'Seattle', 'Tucson']\n",
    "for site in sites:\n",
    "    sent_df = joblib.load(\n",
    "        join(data.PROCESSED_DIR, f'sent_df_{site}_gpt-4.pkl'))\n",
    "    # sent_df = sent_df.sort_values(by=levels, ascending=False)\n",
    "    sent_dfs.append(sent_df)\n",
    "\n",
    "sum_df = pd.concat(sent_dfs).groupby(level=0).sum()\n",
    "\n",
    "# make plot\n",
    "# sum_df = sum_df.sort_values(by=LEVELS, ascending=False)\n",
    "# sum_index_sorted = sum_df.index\n",
    "mean = -2 * sum_df['Very Negative'] + -1 * sum_df['Negative'] + \\\n",
    "    1 * sum_df['Positive'] + 2 * sum_df['Very Positive']\n",
    "sum_index_sorted = mean.sort_values().index\n",
    "sum_df = sum_df.reindex(sum_index_sorted)\n",
    "\n",
    "colors = sns.diverging_palette(20, 220, n=6).as_hex()\n",
    "colors = colors[:2] + ['#ddd', '#eee'] + colors[-2:]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "gs = GridSpec(1, 4, width_ratios=[2, 1, 1, 1], wspace=0.025)\n",
    "\n",
    "\n",
    "# first plot\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "sum_df.plot(kind='barh', stacked=True, color=colors, ax=ax1, legend=False)\n",
    "plt.xlim(0, sum_df.sum(axis=1).max())\n",
    "# remove label for final xtick\n",
    "labels = [str(i) if i % 10 == 0 else '' for i in range(\n",
    "    0, int(sum_df.sum(axis=1).max()) + 1, 10)]\n",
    "labels[-1] = ''\n",
    "plt.xticks(range(0, int(sum_df.sum(axis=1).max()) + 1, 10),\n",
    "           labels=labels)\n",
    "plt.yticks(range(46),\n",
    "           labels=[f'{len(sent_df) - i}. {v}' for i, v in enumerate(\n",
    "               df['Domain'].values[sum_df.index.values])])\n",
    "fig.legend(bbox_to_anchor=(0.5, 0.95), loc='center',\n",
    "           ncol=3, title='Sentiment polarity')\n",
    "plt.xlabel('Answer count')\n",
    "plt.title('All sites', fontweight='bold')\n",
    "plt.ylabel('Question number and domain')\n",
    "\n",
    "sites_examples = ['Atlanta', 'Columbus', 'WashingtonDC']\n",
    "for i, site in enumerate(sites_examples):\n",
    "    ax = fig.add_subplot(gs[i+1])\n",
    "    sent_df = sent_dfs[i]\n",
    "    sent_df = sent_df.reindex(sum_index_sorted)\n",
    "    sent_df.plot(kind='barh', stacked=True, color=colors, ax=ax, legend=False)\n",
    "    plt.xlim(0, sent_df.sum(axis=1).max())\n",
    "    plt.yticks([])\n",
    "    plt.ylabel('')\n",
    "    # plt.title(data.RENAME_SITE_DICT.get(site, site))\n",
    "    plt.title(f'Site {\"ABC\"[i]}')\n",
    "plt.savefig(f'../figs/sentiment_agg.pdf', bbox_inches='tight')\n",
    "plt.savefig(f'../figs/sentiment_agg.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['Domain'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_responded = sum_df.sum().sum() - sum_df['No response'].sum().sum()\n",
    "print(total_responded, 'responses', sum_df['No response'].sum(), 'no response')\n",
    "print('breakdown', sum_df.sum(), round((100 * sum_df.sum() / total_responded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no limit display\n",
    "with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "    d = df.iloc[sum_index_sorted][::-1][['Domain', 'Subcategory']]\n",
    "    d['Question order'] = d.index\n",
    "    d.insert(loc=0, column='Question number', value=np.arange(1, len(d) + 1))\n",
    "    # display(d)\n",
    "    d.to_csv('../figs/question_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # original function filter 10 questions that have many themes (these tend to be more interesting)\n",
    "# # select questions\n",
    "# def get_num_themes(df):\n",
    "#     num_themes_list = []\n",
    "#     for question_num in range(len(df)):\n",
    "#         question, responses, theme_dict = data.get_data_for_question_single_site(\n",
    "#             question_num=question_num, qs=qs, responses_df=responses_df, themes_df=themes_df)\n",
    "#         num_themes_list.append(len(theme_dict))\n",
    "#     return np.array(num_themes_list)\n",
    "#     # df['num_themes'] = num_themes_list\n",
    "#     # return df\n",
    "\n",
    "\n",
    "# num_themes = np.zeros(46)\n",
    "# SITES = ['Atlanta', 'Columbus', 'WashingtonDC']\n",
    "# for site in sites:\n",
    "#     df = files_dict[site]\n",
    "#     num_themes += get_num_themes(df)\n",
    "# idx = pd.Series(num_themes).sort_values(ascending=False)\n",
    "# questions_selected = idx.index[:10]\n",
    "\n",
    "# instead pick 3 most positive, 3 middle, and 3 most negative\n",
    "mid = 46 // 2\n",
    "sorted_qs = list(sum_index_sorted)\n",
    "questions_selected = sorted_qs[:3] + sorted_qs[mid-1:mid+2] + sorted_qs[-3:]\n",
    "\n",
    "# save these questions\n",
    "pd.Series(questions_selected).to_csv(\n",
    "    '../figs/human/sentiment_questions_selected.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select random answers to questions (up to 15 per question)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in responses from all sites\n",
    "resps_dict = defaultdict(list)\n",
    "for question_num in questions_selected:\n",
    "    for site in ['Atlanta', 'Columbus', 'WashingtonDC']:\n",
    "        df = files_dict[site]\n",
    "        question, responses, theme_dict = data.get_data_for_question_single_site(\n",
    "            question_num=question_num, qs=qs, responses_df=responses_df, themes_df=themes_df)\n",
    "        resps_dict[question_num] += list(responses)\n",
    "assert np.all(np.array(list(len(v) for v in resps_dict.values())) == 33)\n",
    "\n",
    "\n",
    "# randomly select up to 15 non-nan responses for each question and record their indices (less if there are fewer than 15)\n",
    "rng = np.random.default_rng(13)\n",
    "resps_idx_selected = defaultdict(list)\n",
    "resps_selected = {}\n",
    "for question_num in questions_selected:\n",
    "    resps = resps_dict[question_num]\n",
    "    indices = np.arange(33)[~pd.isna(resps)]\n",
    "    indices_selected = rng.choice(\n",
    "        indices, size=min(len(indices), 15), replace=False).tolist()\n",
    "    resps_idx_selected[question_num] = indices_selected\n",
    "    resps_selected[question_num] = [resps[i] for i in indices_selected]\n",
    "\n",
    "# put into a big defaultdict\n",
    "dd = defaultdict(list)\n",
    "for question_num in questions_selected:\n",
    "    for i, resp in enumerate(resps_selected[question_num]):\n",
    "        dd['Question number'].append(question_num)\n",
    "        dd['Response number'].append(resps_idx_selected[question_num][i])\n",
    "\n",
    "        dd['Question'].append(qs[question_num])\n",
    "        dd['Response'].append(resp)\n",
    "\n",
    "# dump\n",
    "with open('../figs/human/sentiment_idx_selected.json', 'w') as f:\n",
    "    json.dump(resps_idx_selected, f, indent=4)\n",
    "ddf = pd.DataFrame.from_dict(dd)\n",
    "ddf.to_csv('../figs/human/sentiment_template.csv', index=False)\n",
    "ddf.to_pickle('../figs/human/sentiment_template.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze human results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annots = {\n",
    "    'hum1': 'human1',\n",
    "    'hum2': 'human2',\n",
    "    'hum3': 'human3',\n",
    "}\n",
    "\n",
    "# template = pd.read_pickle('../figs/human/sentiment_template.pkl')\n",
    "template = pd.read_csv('../figs/human/sentiment_template.csv')\n",
    "for k, v in annots.items():\n",
    "    hum = pd.read_csv(f'../figs/human/collected/sentiment_{v}.csv', skiprows=1)\n",
    "\n",
    "    def remove_all_whitespace(s):\n",
    "        return ''.join(s.split())\n",
    "\n",
    "    # check for matching index, value range\n",
    "    assert hum.shape[0] == template.shape[\n",
    "        0], f'Shape mismatch for {k}: {hum.shape[0]} vs {template.shape[0]}'\n",
    "    assert np.all(hum['Response number'].astype(str).apply(remove_all_whitespace).values ==\n",
    "                  template['Response number'].astype(str).apply(remove_all_whitespace).values), f'Error for hum {k}'\n",
    "\n",
    "    # add col\n",
    "    template[k] = hum['Rating'].values.astype(int)\n",
    "    assert np.all(template[k].values >= 1)\n",
    "    assert np.all(template[k].values <= 5)\n",
    "\n",
    "checkpoints_all = [\n",
    "    'gpt-4', 'gpt-35-turbo',\n",
    "    'meta-llama/Llama-2-70b-hf',\n",
    "    'meta-llama/Llama-2-7b-hf',\n",
    "    'mistralai/Mistral-7B-v0.1',  # 'mistralai/Mixtral-8x7B-v0.1'\n",
    "]\n",
    "\n",
    "\n",
    "def find_starting_number(s):\n",
    "    # if s starts with a number or a number with decimal places, return that number\n",
    "    # otherwise return nan\n",
    "    s = str(s).strip()\n",
    "    if s == 'nan':\n",
    "        return np.nan\n",
    "    ans = ''\n",
    "    while len(s) > 0 and (s[0].isdigit() or s[0] == '.'):\n",
    "        ans += s[0]\n",
    "        s = s[1:]\n",
    "    return float(ans) if len(ans) > 0 else np.nan\n",
    "\n",
    "\n",
    "for checkpoint in checkpoints_all:\n",
    "    sites = ['Atlanta', 'Columbus', 'WashingtonDC']\n",
    "    sent_dfs = []\n",
    "    for site in sites:\n",
    "        sent_df = joblib.load(join(\n",
    "            data.PROCESSED_DIR, f'sentiments_df_{site}_{checkpoint.split(\"/\")[-1]}.pkl'))\n",
    "        sent_dfs.append(sent_df)\n",
    "\n",
    "    sent_dfs[0].columns = np.arange(0, 11)\n",
    "    sent_dfs[1].columns = np.arange(11, 22)\n",
    "    sent_dfs[2].columns = np.arange(22, 33)\n",
    "    sent_llm_full = pd.concat(sent_dfs, axis=1).values\n",
    "    template[checkpoint] = template.apply(\n",
    "        lambda row: sent_llm_full[row['Question number'], row['Response number']], axis=1)\n",
    "    template[checkpoint] = template[checkpoint].apply(find_starting_number)\n",
    "llms_to_ensemble = ['gpt-4', 'gpt-35-turbo',\n",
    "                    'mistralai/Mistral-7B-v0.1',  # 'meta-llama/Llama-2-70b-hf',\n",
    "                    # 'mistralai/Mixtral-8x7B-v0.1',\n",
    "                    ]\n",
    "\n",
    "template['Human ensemble'] = template[['hum1', 'hum2', 'hum3']].mean(axis=1)\n",
    "template = template.rename(columns={\n",
    "    'hum1': 'Human 1',\n",
    "    'hum2': 'Human 2',\n",
    "    'hum3': 'Human 3',\n",
    "})\n",
    "\n",
    "\n",
    "template.columns = list(map(lambda x: imodelsx.viz.CHECKPOINTS_RENAME_DICT.get(\n",
    "    x, x), template.columns))\n",
    "checkpoints_all = list(map(lambda x: imodelsx.viz.CHECKPOINTS_RENAME_DICT.get(\n",
    "    x, x), checkpoints_all))\n",
    "llms_to_ensemble = list(map(lambda x: imodelsx.viz.CHECKPOINTS_RENAME_DICT.get(\n",
    "    x, x), llms_to_ensemble))\n",
    "template['LLM ensemble'] = template[llms_to_ensemble].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to apply this across all llms\n",
    "notna = np.ones(len(template), dtype=bool)\n",
    "for ckpt in checkpoints_all:\n",
    "    notna &= template[ckpt].apply(find_starting_number).notna()\n",
    "print('num annots', notna.sum())\n",
    "\n",
    "checkpoints_all_hum = ['Human 1', 'Human 2', 'Human 3', 'Human ensemble'] + \\\n",
    "    ['LLM ensemble'] + checkpoints_all\n",
    "corr = np.zeros((len(checkpoints_all_hum), len(checkpoints_all_hum)))\n",
    "for r, cr in enumerate(checkpoints_all_hum):\n",
    "    for c, cc in enumerate(checkpoints_all_hum):\n",
    "        x = template[cr][notna]\n",
    "        y = template[cc][notna]\n",
    "        corr[r, c] = np.corrcoef(x, y)[0, 1]  # spearmanr(x, y)\n",
    "\n",
    "# convert to df\n",
    "\n",
    "# labels[0] = 'Human 1'\n",
    "# labels[1] = 'Human 2'\n",
    "# labels[2] = 'Human 3'\n",
    "labels = checkpoints_all_hum\n",
    "print(labels, corr.shape)\n",
    "corr_df = pd.DataFrame(corr,\n",
    "                       index=labels,\n",
    "                       columns=labels)\n",
    "\n",
    "# sort by corr with human\n",
    "ind = corr_df.sort_values(by='Human ensemble', ascending=False).index\n",
    "ind.values[:4] = ['Human 1', 'Human 2', 'Human 3', 'Human ensemble']\n",
    "corr_df = corr_df.reindex(ind)[ind]\n",
    "\n",
    "# Replace correlations with Human ensemble by excluding the human\n",
    "cols = ['Human 1', 'Human 2', 'Human 3']\n",
    "hum_corrs = []\n",
    "for i, c in enumerate(cols):\n",
    "    avg_excluding_c = template[[col for col in cols if col != c]].mean(axis=1)\n",
    "    hum_corrs.append(np.corrcoef(\n",
    "        template[c][notna], avg_excluding_c[notna])[0, 1])\n",
    "# print(hum_corrs)\n",
    "corr_df.loc[cols, 'Human ensemble'] = hum_corrs\n",
    "corr_df.loc['Human ensemble', cols] = hum_corrs\n",
    "\n",
    "# Replace correlations with LLM ensemble by excluding the LLM\n",
    "cols = list(map(lambda x: imodelsx.viz.CHECKPOINTS_RENAME_DICT.get(\n",
    "    x, x), llms_to_ensemble))\n",
    "llm_corrs = []\n",
    "for i, c in enumerate(cols):\n",
    "    avg_excluding_c = template[[col for col in cols if col != c]].mean(axis=1)\n",
    "    llm_corrs.append(np.corrcoef(\n",
    "        template[c][notna], avg_excluding_c[notna])[0, 1])\n",
    "# print(llm_corrs)\n",
    "corr_df.loc[cols, 'LLM ensemble'] = llm_corrs\n",
    "corr_df.loc['LLM ensemble', cols] = llm_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_confidence_interval(r, alpha, n):\n",
    "    def _r_to_z(r):\n",
    "        return math.log((1 + r) / (1 - r)) / 2.0\n",
    "\n",
    "    def _z_to_r(z):\n",
    "        e = math.exp(2 * z)\n",
    "        return ((e - 1) / (e + 1))\n",
    "\n",
    "    z = _r_to_z(r)\n",
    "    se = 1.0 / math.sqrt(n - 3)\n",
    "    z_crit = scipy.stats.norm.ppf(1 - alpha/2)  # 2-tailed z critical value\n",
    "\n",
    "    lo = z - z_crit * se\n",
    "    hi = z + z_crit * se\n",
    "\n",
    "    # Return a sequence\n",
    "    return (round(_z_to_r(lo), 2), round(_z_to_r(hi), 2))\n",
    "\n",
    "\n",
    "print('interval human-LLM', r_confidence_interval(0.741484, 0.05, 123))\n",
    "print('interval annotators', np.mean([0.813302, 0.850014, 0.935228]), r_confidence_interval(\n",
    "    np.mean([0.813302, 0.850014, 0.935228]), 0.05, 123))\n",
    "print('interval each human-LLM',\n",
    "      [r_confidence_interval(x, 0.05, 123) for x in [0.845929, 0.915878, 0.941113]])\n",
    "print('interval human1-human2',\n",
    "      [r_confidence_interval(0.813302, 0.05, 123)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_df = corr_df.iloc[1:, 1:]\n",
    "mask = np.triu(np.ones_like(corr_df, dtype=bool))\n",
    "mask[np.diag_indices_from(mask)] = False\n",
    "\n",
    "rename = {\n",
    "    'Human 1': 'Human reviewer 1',\n",
    "    'Human 2': 'Human reviewer 2',\n",
    "    'Human 3': 'Human reviewer 3',\n",
    "    'Human ensemble': 'Human reviewer ensemble',\n",
    "}\n",
    "corr_df = corr_df.rename(columns=rename, index=rename)\n",
    "ax = sns.heatmap(\n",
    "    corr_df,\n",
    "    annot=True, fmt='.2f',\n",
    "    cmap=sns.color_palette(\"Blues\", as_cmap=True), cbar_kws={'label': 'Correlation'},\n",
    "    mask=mask,\n",
    ")\n",
    "\n",
    "# outline the first row of the elements in the heatmap\n",
    "color = '#fa755a'\n",
    "lw = 3\n",
    "roffset = 3.5\n",
    "coffset = 0.5\n",
    "shape = corr_df.shape\n",
    "r = 0\n",
    "# color = 'gray'\n",
    "alpha = 1\n",
    "for c in range(3, shape[1]):\n",
    "    rx = r + roffset\n",
    "    cx = c + coffset\n",
    "    if c == 2:\n",
    "        plt.plot([rx - 0.5, rx + 0.5],\n",
    "                 [cx - 0.5, cx - 0.5], color=color, lw=lw, alpha=alpha)\n",
    "    if c == shape[1] - 1:\n",
    "        plt.plot([rx - 0.5, rx + 0.5],\n",
    "                 [cx + 0.5, cx + 0.5], color=color, lw=lw, alpha=alpha)\n",
    "    plt.plot([rx - 0.5, rx - 0.5],\n",
    "             [cx - 0.5, cx + 0.5], color=color, lw=lw, alpha=alpha)\n",
    "    plt.plot([rx + 0.5, rx + 0.5],\n",
    "             [cx - 0.5, cx + 0.5], color=color, lw=lw, alpha=alpha)\n",
    "\n",
    "roffset = 0.5\n",
    "coffset = 0.5\n",
    "c = 3\n",
    "for r in range(0, 4):\n",
    "    kwargs = {\n",
    "        'color': color,\n",
    "        'lw': lw,\n",
    "        # 'linestyle': '-',\n",
    "        # 'marker': 'None',\n",
    "        # 'alpha': 0.5,\n",
    "    }\n",
    "    rx = r + roffset\n",
    "    cx = c + coffset\n",
    "    # if c == 2:\n",
    "    plt.plot([rx - 0.5, rx + 0.5],\n",
    "             [cx - 0.5, cx - 0.5], **kwargs)\n",
    "    # if c == shape[1] - 1:\n",
    "    plt.plot([rx - 0.5, rx + 0.5],\n",
    "             [cx + 0.5, cx + 0.5], **kwargs)\n",
    "    if r == 0:\n",
    "        plt.plot([rx - 0.5, rx - 0.5],\n",
    "                 [cx - 0.5, cx + 0.5], **kwargs)\n",
    "    if r == 3:\n",
    "        plt.plot([rx + 0.5, rx + 0.5],\n",
    "                 [cx - 0.5, cx + 0.5], **kwargs)\n",
    "\n",
    "# set the color of first three xticklabels and yticklabels to blue\n",
    "for i, t in enumerate(ax.get_xticklabels()):\n",
    "    if i < 4:\n",
    "        t.set_color('#08346c')\n",
    "for i, t in enumerate(ax.get_yticklabels()):\n",
    "    if i < 4:\n",
    "        t.set_color('#08346c')\n",
    "\n",
    "\n",
    "plt.xlim(-.2, shape[0])\n",
    "plt.ylim(shape[1] + 0.2, -.2)\n",
    "# plt.ylabel('Annotator')\n",
    "# plt.xlabel('Annotator')\n",
    "plt.savefig('../figs/sentiment_correlation.pdf', bbox_inches='tight')\n",
    "plt.savefig('../figs/sentiment_correlation.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = corr_df.loc['Annotator ensemble', [\n",
    "    'Annotator 1', 'Annotator 2', 'Annotator 3']]\n",
    "print('Human ensemble vs human', vals.mean(), vals.sem(ddof=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = template['LLM ensemble'][notna]\n",
    "x2 = template['Human ensemble'][notna]\n",
    "\n",
    "# calculate pearson correlation and error of the corr\n",
    "corr = np.corrcoef(x1, x2)[0, 1]\n",
    "err = np.sqrt((1 - corr**2) / (len(x1) - 2))\n",
    "print('LLM ensemble vs human ensemble corr', corr, 'err', err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with some jitter\n",
    "def jitter(values, j, min=1, max=5):\n",
    "    values = values + np.random.uniform(-j, j, values.shape)\n",
    "    return np.clip(values, min, max)\n",
    "\n",
    "\n",
    "x = template['Human ensemble']\n",
    "y = template['LLM ensemble']\n",
    "plt.plot(jitter(x, 0.15), jitter(y, 0.15), 'o', alpha=0.5)\n",
    "plt.xlabel('Annotator ensemble sentiment score')\n",
    "plt.ylabel('LLM ensemble sentiment score')\n",
    "\n",
    "plt.plot([1, 5], [1, 5], 'k--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fraction of time LLM is more extreme than human',\n",
    "      (np.abs(y[notna] - 3) > np.abs(x[notna] - 3)).mean())\n",
    "print('Fraction of time LLM reverses polarity',\n",
    "      (((x >= 4) & (y <= 2)) | ((x <= 2) & (y >= 4)))[notna].mean()\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
